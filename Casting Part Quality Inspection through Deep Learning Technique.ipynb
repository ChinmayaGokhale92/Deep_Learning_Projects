{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfe3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc18c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\ALL Python ML notebooks\\\\Deep Learning Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7824ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f65756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\ALL Python ML notebooks\\\\Deep Learning Notebooks\\\\Casting Data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e2119",
   "metadata": {},
   "source": [
    "### Casting Product Image Data For Quality Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7028c8",
   "metadata": {},
   "source": [
    "* Convolutional neural networks (CNN) is used to convert digital image content into a single vector of numbers(numeric vector) representing the unique characteristics of the image. \n",
    "* The column of numbers is input to a dense fully connected Neural Network layer against the labels, which image is cat, which image is bird etc.\n",
    "* The classification model learns these numeric vector inputs against the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e825712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[image.png]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473011d",
   "metadata": {},
   "source": [
    "### Reading the Images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9125ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning CNN model to recognise mechanical part which is defective/okay\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not''''''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    "\n",
    "'''########################## IMAGE PRE-PROCESSING for TRAINING and TESTING data ##############################'''\n",
    "\n",
    "TrainingImagePath = 'C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data/casting_data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "660d7391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# ImageDataGenerator is the function which tries to create even more data with respect to existing raw images\n",
    "# To Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56dc9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformation on raw images of training data\n",
    "# This ImageDataGenerator tries to generate more data than existed\n",
    "# train datagen will have these kind of the properties\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1/255, # Rescale to 255 pixels\n",
    "        shear_range=0.1, # Shear/cut it by 10%\n",
    "        zoom_range=0.1, # Zoom it by 10%\n",
    "        horizontal_flip=True) # Flip it horizontally as well,\n",
    "# It applies above operations on the available images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a17ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# In testing we are not performing any operation\n",
    "# In testing we are not going to manipulate the image lie rescaling, shear range, zoom, etc.\n",
    "# We just input the image as it is..in testing dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdbc89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6633 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating the Training Data generated by using TrainingDataGenerator object\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64), #Usually we have 331x331 pixels image which are more enough for the system to process.\n",
    "                            # Essentially these algorithms are helpful to preserve main characteristic of images, still\n",
    "                            # generate a good image, here we compress it and learn these 331 pixels\n",
    "                            # Now CNN has reinforced it, compress it and extracting and learning it..\n",
    "                            # Just to avoid CPU load, we are reducing the image spec, which is good in recognising\n",
    "                            # If we need, we can give 200x200 but it should be as lesser as possible\n",
    "        batch_size=32, # batch size is 32, in how many batches we are going to do operation\n",
    "        class_mode='categorical') # It maintains an index, face1->index 0, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff410513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6633 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64,64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6aab6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'def_front': 0, 'ok_front': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing class labels for each product photo\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141820cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efcd7da",
   "metadata": {},
   "source": [
    "### Creating a list of faces and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fbb2aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Part and its ID {0: 'def_front', 1: 'ok_front'}\n",
      "\\ The Number of output neurons:  2\n"
     ]
    }
   ],
   "source": [
    "'''################ Creating lookup table for all photos###########'''\n",
    "# class_indices have the numeric tag for each photo\n",
    "TrainClasses=training_set.class_indices\n",
    "\n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for partValue, partName in zip(TrainClasses.values(), TrainClasses.keys()):\n",
    "    ResultMap[partValue]=partName\n",
    "    \n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultMap.pkl\", 'wb') as f: # here we storing it\n",
    "    pickle.dump(ResultMap, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('Mapping of Part and its ID', ResultMap)\n",
    "\n",
    "# The number of neurons for the output layer is equal to the number of parts\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\ The Number of output neurons: ', OutputNeurons) # Output layer is determined by number of classes, 2 probabilities\n",
    "# No. of output neurons are 2, output produced by ANN is 2 probabilities out of which highest probabilities will win"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e977f",
   "metadata": {},
   "source": [
    "### Creating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db6d83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''############## Create CNN Deep Learning model ##########'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    "\n",
    "''' STEP--1 CONVOLUTION\n",
    "# Adding the first layer of CNN\n",
    "# We are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64x64) pixels representing Red, Green and Blue components of pixels'''\n",
    "classifier.add(Convolution2D(filters=32, kernel_size=(3,3), strides=(1,1), input_shape=(64,64,3),# Means 3 Matrices of 64x64\n",
    "                            activation='relu'))\n",
    "\n",
    "'''# STEP--2 MAX Pooling'''# Hyper parameter-> pool size which affects accuracy\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''############ ADDITIONAL LAYER of CONVOLUTION for better accuracy ##########'''\n",
    "# Additional round of convule/consize/reduce image size-> it extracts most important part of the image\n",
    "classifier.add(Convolution2D(filters=64, kernel_size=(5,5), strides=(2,2), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "'''# STEP--3 FLATTEING'''\n",
    "classifier.add(Flatten())\n",
    "\n",
    "'''# STEP--4 FULLY CONNECTED NEURAL NETWORK'''\n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    "\n",
    "# Output layer\n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    "\n",
    "'''# COMPILING THE CNN'''\n",
    "#classfier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "# crossentropy--> categorical_crossentropy for multiclass classification\n",
    "# binary_crossentropy--> for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a657b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f187f79",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7437dfe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2132\\2664104734.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  classifier.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "5/5 [==============================] - 2s 462ms/step - loss: 0.6855 - accuracy: 0.5625 - val_loss: 0.6942 - val_accuracy: 0.5312\n",
      "Epoch 2/80\n",
      "5/5 [==============================] - 2s 419ms/step - loss: 0.6749 - accuracy: 0.6000 - val_loss: 0.7162 - val_accuracy: 0.4531\n",
      "Epoch 3/80\n",
      "5/5 [==============================] - 2s 411ms/step - loss: 0.6907 - accuracy: 0.5437 - val_loss: 0.6634 - val_accuracy: 0.6406\n",
      "Epoch 4/80\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.6785 - accuracy: 0.5875 - val_loss: 0.6767 - val_accuracy: 0.5938\n",
      "Epoch 5/80\n",
      "5/5 [==============================] - 2s 447ms/step - loss: 0.6907 - accuracy: 0.5437 - val_loss: 0.6854 - val_accuracy: 0.5625\n",
      "Epoch 6/80\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6854 - val_accuracy: 0.5625\n",
      "Epoch 7/80\n",
      "5/5 [==============================] - 3s 691ms/step - loss: 0.6889 - accuracy: 0.5500 - val_loss: 0.6681 - val_accuracy: 0.6250\n",
      "Epoch 8/80\n",
      "5/5 [==============================] - 2s 488ms/step - loss: 0.6940 - accuracy: 0.5312 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
      "Epoch 9/80\n",
      "5/5 [==============================] - 2s 406ms/step - loss: 0.6666 - accuracy: 0.6313 - val_loss: 0.6854 - val_accuracy: 0.5625\n",
      "Epoch 10/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6923 - accuracy: 0.5375 - val_loss: 0.6811 - val_accuracy: 0.5781\n",
      "Epoch 11/80\n",
      "5/5 [==============================] - 2s 414ms/step - loss: 0.7093 - accuracy: 0.4750 - val_loss: 0.6811 - val_accuracy: 0.5781\n",
      "Epoch 12/80\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.6786 - accuracy: 0.5875 - val_loss: 0.6646 - val_accuracy: 0.6406\n",
      "Epoch 13/80\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.6754 - accuracy: 0.6000 - val_loss: 0.6977 - val_accuracy: 0.5156\n",
      "Epoch 14/80\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.6969 - accuracy: 0.5188 - val_loss: 0.6853 - val_accuracy: 0.5625\n",
      "Epoch 15/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6886 - accuracy: 0.5500 - val_loss: 0.6853 - val_accuracy: 0.5625\n",
      "Epoch 16/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6934 - accuracy: 0.5312 - val_loss: 0.7133 - val_accuracy: 0.4531\n",
      "Epoch 17/80\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.6789 - accuracy: 0.5875 - val_loss: 0.6973 - val_accuracy: 0.5156\n",
      "Epoch 18/80\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.6805 - accuracy: 0.5813 - val_loss: 0.6933 - val_accuracy: 0.5312\n",
      "Epoch 19/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6901 - accuracy: 0.5437 - val_loss: 0.6576 - val_accuracy: 0.6719\n",
      "Epoch 20/80\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.6806 - accuracy: 0.5813 - val_loss: 0.6853 - val_accuracy: 0.5625\n",
      "Epoch 21/80\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.6949 - accuracy: 0.5250 - val_loss: 0.6932 - val_accuracy: 0.5312\n",
      "Epoch 22/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6995 - accuracy: 0.5063 - val_loss: 0.6853 - val_accuracy: 0.5625\n",
      "Epoch 23/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6900 - accuracy: 0.5437 - val_loss: 0.7043 - val_accuracy: 0.4844\n",
      "Epoch 24/80\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.6673 - accuracy: 0.6375 - val_loss: 0.6815 - val_accuracy: 0.5781\n",
      "Epoch 25/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6974 - accuracy: 0.5125 - val_loss: 0.6853 - val_accuracy: 0.5625\n",
      "Epoch 26/80\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.6778 - accuracy: 0.5938 - val_loss: 0.6625 - val_accuracy: 0.6562\n",
      "Epoch 27/80\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.6685 - accuracy: 0.6313 - val_loss: 0.6659 - val_accuracy: 0.6406\n",
      "Epoch 28/80\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.6869 - accuracy: 0.5562 - val_loss: 0.6853 - val_accuracy: 0.5625\n",
      "Epoch 29/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6662 - accuracy: 0.6375 - val_loss: 0.6853 - val_accuracy: 0.5625\n",
      "Epoch 30/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6887 - accuracy: 0.5500 - val_loss: 0.6936 - val_accuracy: 0.5312\n",
      "Epoch 31/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6671 - accuracy: 0.6313 - val_loss: 0.6727 - val_accuracy: 0.6094\n",
      "Epoch 32/80\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.6751 - accuracy: 0.6000 - val_loss: 0.6768 - val_accuracy: 0.5938\n",
      "Epoch 33/80\n",
      "5/5 [==============================] - 2s 367ms/step - loss: 0.6802 - accuracy: 0.5813 - val_loss: 0.6854 - val_accuracy: 0.5625\n",
      "Epoch 34/80\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6890 - accuracy: 0.5500 - val_loss: 0.6722 - val_accuracy: 0.6094\n",
      "Epoch 35/80\n",
      "5/5 [==============================] - 2s 446ms/step - loss: 0.6908 - accuracy: 0.5437 - val_loss: 0.6678 - val_accuracy: 0.6250\n",
      "Epoch 36/80\n",
      "5/5 [==============================] - 2s 440ms/step - loss: 0.6837 - accuracy: 0.5688 - val_loss: 0.7031 - val_accuracy: 0.5000\n",
      "Epoch 37/80\n",
      "5/5 [==============================] - 2s 426ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.7030 - val_accuracy: 0.5000\n",
      "Epoch 38/80\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.6660 - accuracy: 0.6313 - val_loss: 0.6988 - val_accuracy: 0.5156\n",
      "Epoch 39/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6783 - accuracy: 0.5875 - val_loss: 0.6720 - val_accuracy: 0.6094\n",
      "Epoch 40/80\n",
      "5/5 [==============================] - 2s 367ms/step - loss: 0.6819 - accuracy: 0.5750 - val_loss: 0.6945 - val_accuracy: 0.5312\n",
      "Epoch 41/80\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6801 - accuracy: 0.5813 - val_loss: 0.7129 - val_accuracy: 0.4688\n",
      "Epoch 42/80\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6819 - accuracy: 0.5750 - val_loss: 0.6947 - val_accuracy: 0.5312\n",
      "Epoch 43/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6965 - accuracy: 0.5250 - val_loss: 0.6992 - val_accuracy: 0.5156\n",
      "Epoch 44/80\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.6564 - accuracy: 0.6625 - val_loss: 0.6993 - val_accuracy: 0.5156\n",
      "Epoch 45/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6745 - accuracy: 0.6000 - val_loss: 0.7184 - val_accuracy: 0.4531\n",
      "Epoch 46/80\n",
      "5/5 [==============================] - 2s 418ms/step - loss: 0.6876 - accuracy: 0.5562 - val_loss: 0.6619 - val_accuracy: 0.6406\n",
      "Epoch 47/80\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.6838 - accuracy: 0.5688 - val_loss: 0.6714 - val_accuracy: 0.6094\n",
      "Epoch 48/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6933 - accuracy: 0.5375 - val_loss: 0.6999 - val_accuracy: 0.5156\n",
      "Epoch 49/80\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.6781 - accuracy: 0.5875 - val_loss: 0.6809 - val_accuracy: 0.5781\n",
      "Epoch 50/80\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.7008 - accuracy: 0.5125 - val_loss: 0.6531 - val_accuracy: 0.6719\n",
      "Epoch 51/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.7002 - accuracy: 0.5125 - val_loss: 0.6810 - val_accuracy: 0.5781\n",
      "Epoch 52/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6890 - accuracy: 0.5500 - val_loss: 0.6454 - val_accuracy: 0.7031\n",
      "Epoch 53/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6942 - accuracy: 0.5312 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
      "Epoch 54/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6681 - accuracy: 0.6250 - val_loss: 0.6725 - val_accuracy: 0.6094\n",
      "Epoch 55/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6716 - accuracy: 0.6125 - val_loss: 0.6811 - val_accuracy: 0.5781\n",
      "Epoch 56/80\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.6802 - accuracy: 0.5813 - val_loss: 0.6591 - val_accuracy: 0.6562\n",
      "Epoch 57/80\n",
      "5/5 [==============================] - 2s 406ms/step - loss: 0.6977 - accuracy: 0.5188 - val_loss: 0.6548 - val_accuracy: 0.6719\n",
      "Epoch 58/80\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.6837 - accuracy: 0.5688 - val_loss: 0.6638 - val_accuracy: 0.6406\n",
      "Epoch 59/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6820 - accuracy: 0.5750 - val_loss: 0.6767 - val_accuracy: 0.5938\n",
      "Epoch 60/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6785 - accuracy: 0.5875 - val_loss: 0.6985 - val_accuracy: 0.5156\n",
      "Epoch 61/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6723 - val_accuracy: 0.6094\n",
      "Epoch 62/80\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6644 - accuracy: 0.6375 - val_loss: 0.6634 - val_accuracy: 0.6406\n",
      "Epoch 63/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6695 - accuracy: 0.6187 - val_loss: 0.7036 - val_accuracy: 0.5000\n",
      "Epoch 64/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6837 - accuracy: 0.5688 - val_loss: 0.6441 - val_accuracy: 0.7031\n",
      "Epoch 65/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.7150 - accuracy: 0.4625 - val_loss: 0.6992 - val_accuracy: 0.5156\n",
      "Epoch 66/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6855 - accuracy: 0.5625 - val_loss: 0.6855 - val_accuracy: 0.5625\n",
      "Epoch 67/80\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
      "Epoch 68/80\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.6872 - accuracy: 0.5562 - val_loss: 0.6985 - val_accuracy: 0.5156\n",
      "Epoch 69/80\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.6906 - accuracy: 0.5437 - val_loss: 0.7027 - val_accuracy: 0.5000\n",
      "Epoch 70/80\n",
      "5/5 [==============================] - 2s 328ms/step - loss: 0.6734 - accuracy: 0.6058 - val_loss: 0.6897 - val_accuracy: 0.5469\n",
      "Epoch 71/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6956 - accuracy: 0.5250 - val_loss: 0.6684 - val_accuracy: 0.6250\n",
      "Epoch 72/80\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.6887 - accuracy: 0.5500 - val_loss: 0.6643 - val_accuracy: 0.6406\n",
      "Epoch 73/80\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.6920 - accuracy: 0.5375 - val_loss: 0.7060 - val_accuracy: 0.4844\n",
      "Epoch 74/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6756 - accuracy: 0.6000 - val_loss: 0.6690 - val_accuracy: 0.6250\n",
      "Epoch 75/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6723 - accuracy: 0.6125 - val_loss: 0.7100 - val_accuracy: 0.4688\n",
      "Epoch 76/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6937 - accuracy: 0.5312 - val_loss: 0.6812 - val_accuracy: 0.5781\n",
      "Epoch 77/80\n",
      "5/5 [==============================] - 2s 375ms/step - loss: 0.6904 - accuracy: 0.5437 - val_loss: 0.6937 - val_accuracy: 0.5312\n",
      "Epoch 78/80\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.6770 - accuracy: 0.5938 - val_loss: 0.7104 - val_accuracy: 0.4688\n",
      "Epoch 79/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
      "Epoch 80/80\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.6804 - accuracy: 0.5813 - val_loss: 0.6937 - val_accuracy: 0.5312\n",
      "############# Total Time Taken:  3 Minutes ########\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    "\n",
    "# Starting the model training\n",
    "classifier.fit_generator(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=5,\n",
    "                    epochs=80, # like batch size for you, change epoch size to optimize the accuracy\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=2)\n",
    "\n",
    "EndTime=time.time()\n",
    "print('############# Total Time Taken: ', round((EndTime-StartTime)/60), 'Minutes ########')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ab77e",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "060b3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data\\assets\n"
     ]
    }
   ],
   "source": [
    "### Saving the model\n",
    "classifier.save('C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b203d0",
   "metadata": {},
   "source": [
    "### Testing the model on a different part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f6f0e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5663694  0.43363056]]\n",
      "{0: 'def_front', 1: 'ok_front'}\n",
      "########################################\n",
      "Prediction is:  def_front\n"
     ]
    }
   ],
   "source": [
    "'''############## Making single predictions #############'''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "testImage='C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data/casting_data/test/def_front/cast_def_0_143.jpeg'\n",
    "test_image=image.load_img(testImage,target_size=(64,64)) # Converts image to array object like nuerical array\n",
    "test_image=image.img_to_array(test_image) # the image 64,64 has to be converted into array format\n",
    "\n",
    "test_image=np.expand_dims(test_image,axis=0) # 64x64x3 array is created\n",
    "# This image is at the same level, where we trained our model\n",
    "\n",
    "result=classifier.predict(test_image,verbose=0) # Passing test image to classifier using predict function\n",
    "# verbose is 0 which doesn't produce the log output\n",
    "\n",
    "print(result)\n",
    "print(ResultMap)\n",
    "\n",
    "print('####'*10)\n",
    "print('Prediction is: ', ResultMap[np.argmax(result)]) # which is the max value give me the location of the value\n",
    "# whichever is maximum, it gives the location of that value , from which we get index, result map of highest value will\n",
    "# give us the part number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b89c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
