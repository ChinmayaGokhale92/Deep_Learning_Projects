{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfe3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc18c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\ALL Python ML notebooks\\\\Deep Learning Notebooks\\\\Casting Data'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7824ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47f65756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\ALL Python ML notebooks\\\\Deep Learning Notebooks\\\\Casting Data'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e2119",
   "metadata": {},
   "source": [
    "### Casting Product Image Data For Quality Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7028c8",
   "metadata": {},
   "source": [
    "* Convolutional neural networks (CNN) is used to convert digital image content into a single vector of numbers(numeric vector) representing the unique characteristics of the image. \n",
    "* The column of numbers is input to a dense fully connected Neural Network layer against the labels, which image is cat, which image is bird etc.\n",
    "* The classification model learns these numeric vector inputs against the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e825712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[image.png]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473011d",
   "metadata": {},
   "source": [
    "### Reading the Images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb9125ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning CNN model to recognise mechanical part which is defective/okay\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not''''''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    "\n",
    "'''########################## IMAGE PRE-PROCESSING for TRAINING and TESTING data ##############################'''\n",
    "\n",
    "TrainingImagePath = 'C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data/casting_data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "660d7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# ImageDataGenerator is the function which tries to create even more data with respect to existing raw images\n",
    "# To Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56dc9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformation on raw images of training data\n",
    "# This ImageDataGenerator tries to generate more data than existed\n",
    "# train datagen will have these kind of the properties\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1/255, # Rescale to 255 pixels\n",
    "        shear_range=0.1, # Shear/cut it by 10%\n",
    "        zoom_range=0.1, # Zoom it by 10%\n",
    "        horizontal_flip=True) # Flip it horizontally as well,\n",
    "# It applies above operations on the available images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a17ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# In testing we are not performing any operation\n",
    "# In testing we are not going to manipulate the image lie rescaling, shear range, zoom, etc.\n",
    "# We just input the image as it is..in testing dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdbc89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6633 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating the Training Data generated by using TrainingDataGenerator object\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64), #Usually we have 331x331 pixels image which are more enough for the system to process.\n",
    "                            # Essentially these algorithms are helpful to preserve main characteristic of images, still\n",
    "                            # generate a good image, here we compress it and learn these 331 pixels\n",
    "                            # Now CNN has reinforced it, compress it and extracting and learning it..\n",
    "                            # Just to avoid CPU load, we are reducing the image spec, which is good in recognising\n",
    "                            # If we need, we can give 200x200 but it should be as lesser as possible\n",
    "        batch_size=32, # batch size is 32, in how many batches we are going to do operation\n",
    "        class_mode='categorical') # It maintains an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff410513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6633 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64,64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6aab6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'def_front': 0, 'ok_front': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing class labels for each product photo\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141820cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efcd7da",
   "metadata": {},
   "source": [
    "### Creating a list of faces and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fbb2aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Part and its ID {0: 'def_front', 1: 'ok_front'}\n",
      "\\ The Number of output neurons:  2\n"
     ]
    }
   ],
   "source": [
    "'''################ Creating lookup table for all photos###########'''\n",
    "# class_indices have the numeric tag for each photo\n",
    "TrainClasses=training_set.class_indices\n",
    "\n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for partValue, partName in zip(TrainClasses.values(), TrainClasses.keys()):\n",
    "    ResultMap[partValue]=partName\n",
    "    \n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultMap.pkl\", 'wb') as f: # here we storing it\n",
    "    pickle.dump(ResultMap, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('Mapping of Part and its ID', ResultMap)\n",
    "\n",
    "# The number of neurons for the output layer is equal to the number of parts\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\ The Number of output neurons: ', OutputNeurons) # Output layer is determined by number of classes, 2 probabilities\n",
    "# No. of output neurons are 2, output produced by ANN is 2 probabilities out of which highest probabilities will win"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e977f",
   "metadata": {},
   "source": [
    "### Creating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db6d83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''############## Create CNN Deep Learning model ##########'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    "\n",
    "''' STEP--1 CONVOLUTION\n",
    "# Adding the first layer of CNN\n",
    "# We are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64x64) pixels representing Red, Green and Blue components of pixels'''\n",
    "classifier.add(Convolution2D(filters=32, kernel_size=(3,3), strides=(1,1), input_shape=(64,64,3),# Means 3 Matrices of 64x64\n",
    "                            activation='relu'))\n",
    "\n",
    "'''# STEP--2 MAX Pooling'''# Hyper parameter-> pool size which affects accuracy\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''############ ADDITIONAL LAYER of CONVOLUTION for better accuracy ##########'''\n",
    "# Additional round of convule/consize/reduce image size-> it extracts most important part of the image\n",
    "classifier.add(Convolution2D(filters=64, kernel_size=(5,5), strides=(2,2), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "'''# STEP--3 FLATTEING'''\n",
    "classifier.add(Flatten())\n",
    "\n",
    "'''# STEP--4 FULLY CONNECTED NEURAL NETWORK'''\n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    "\n",
    "# Output layer\n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    "\n",
    "'''# COMPILING THE CNN'''\n",
    "#classfier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "# crossentropy--> categorical_crossentropy for multiclass classification\n",
    "# binary_crossentropy--> for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a657b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f187f79",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7437dfe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2132\\1992972017.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  classifier.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 4s 577ms/step - loss: 0.7130 - accuracy: 0.5938 - val_loss: 0.6813 - val_accuracy: 0.5813\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 2s 476ms/step - loss: 0.7112 - accuracy: 0.4187 - val_loss: 0.6966 - val_accuracy: 0.4125\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 2s 487ms/step - loss: 0.6779 - accuracy: 0.6062 - val_loss: 0.6793 - val_accuracy: 0.5625\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 2s 486ms/step - loss: 0.6807 - accuracy: 0.5562 - val_loss: 0.6621 - val_accuracy: 0.6125\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 2s 485ms/step - loss: 0.6947 - accuracy: 0.5125 - val_loss: 0.6728 - val_accuracy: 0.5562\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 2s 469ms/step - loss: 0.6604 - accuracy: 0.6125 - val_loss: 0.6617 - val_accuracy: 0.5875\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.6796 - accuracy: 0.5250 - val_loss: 0.6627 - val_accuracy: 0.5875\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 2s 493ms/step - loss: 0.6621 - accuracy: 0.5938 - val_loss: 0.6489 - val_accuracy: 0.5875\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 2s 491ms/step - loss: 0.6519 - accuracy: 0.5750 - val_loss: 0.6587 - val_accuracy: 0.7000\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 3s 595ms/step - loss: 0.6592 - accuracy: 0.6562 - val_loss: 0.6266 - val_accuracy: 0.7437\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 2s 493ms/step - loss: 0.6624 - accuracy: 0.6062 - val_loss: 0.6263 - val_accuracy: 0.7563\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 2s 477ms/step - loss: 0.6412 - accuracy: 0.6687 - val_loss: 0.6267 - val_accuracy: 0.6938\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.6198 - accuracy: 0.6875 - val_loss: 0.5805 - val_accuracy: 0.6438\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 2s 485ms/step - loss: 0.6287 - accuracy: 0.6000 - val_loss: 0.6047 - val_accuracy: 0.7000\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 2s 487ms/step - loss: 0.6458 - accuracy: 0.6496 - val_loss: 0.6135 - val_accuracy: 0.6625\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 2s 505ms/step - loss: 0.6221 - accuracy: 0.6438 - val_loss: 0.5737 - val_accuracy: 0.7437\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 2s 482ms/step - loss: 0.6051 - accuracy: 0.6938 - val_loss: 0.5742 - val_accuracy: 0.7500\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.5678 - accuracy: 0.7312 - val_loss: 0.5666 - val_accuracy: 0.6625\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 2s 518ms/step - loss: 0.5946 - accuracy: 0.7188 - val_loss: 0.5593 - val_accuracy: 0.7750\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.5793 - accuracy: 0.7153 - val_loss: 0.6034 - val_accuracy: 0.6562\n",
      "############# Total Time Taken:  1 Minutes ########\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    "\n",
    "# Starting the model training\n",
    "classifier.fit_generator(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=5,\n",
    "                    epochs=20, # like batch size for you, change epoch size to optimize the accuracy\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=5)\n",
    "\n",
    "EndTime=time.time()\n",
    "print('############# Total Time Taken: ', round((EndTime-StartTime)/60), 'Minutes ########')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ab77e",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "060b3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data\\assets\n"
     ]
    }
   ],
   "source": [
    "### Saving the model\n",
    "classifier.save('C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b203d0",
   "metadata": {},
   "source": [
    "### Testing the model on a different part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f6f0e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2053294 0.7946706]]\n",
      "{0: 'def_front', 1: 'ok_front'}\n",
      "########################################\n",
      "Prediction is:  ok_front\n"
     ]
    }
   ],
   "source": [
    "'''############## Making single predictions #############'''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "testImage='C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data/casting_data/test/ok_front/cast_ok_0_259.jpeg'\n",
    "test_image=image.load_img(testImage,target_size=(64,64)) # Converts image to array object like nuerical array\n",
    "test_image=image.img_to_array(test_image) # the image 64,64 has to be converted into array format\n",
    "\n",
    "test_image=np.expand_dims(test_image,axis=0) # 64x64x3 array is created\n",
    "# This image is at the same level, where we trained our model\n",
    "\n",
    "result=classifier.predict(test_image,verbose=0) # Passing test image to classifier using predict function\n",
    "# verbose is 0 which doesn't produce the log output\n",
    "\n",
    "print(result)\n",
    "print(ResultMap)\n",
    "\n",
    "print('####'*10)\n",
    "print('Prediction is: ', ResultMap[np.argmax(result)]) # which is the max value give me the location of the value\n",
    "# whichever is maximum, it gives the location of that value , from which we get index, result map of highest value will\n",
    "# give us the part number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b89c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
