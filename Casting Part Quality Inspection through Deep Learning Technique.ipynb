{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfe3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc18c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\ALL Python ML notebooks\\\\Deep Learning Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7824ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f65756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo\\\\ALL Python ML notebooks\\\\Deep Learning Notebooks\\\\Casting Data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e2119",
   "metadata": {},
   "source": [
    "### Casting Product Image Data For Quality Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7028c8",
   "metadata": {},
   "source": [
    "* Convolutional neural networks (CNN) is used to convert digital image content into a single vector of numbers(numeric vector) representing the unique characteristics of the image. \n",
    "* The column of numbers is input to a dense fully connected Neural Network layer against the labels, which image is cat, which image is bird etc.\n",
    "* The classification model learns these numeric vector inputs against the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e825712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[image.png]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473011d",
   "metadata": {},
   "source": [
    "### Reading the Images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9125ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning CNN model to recognise mechanical part which is defective/okay\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not''''''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    "\n",
    "'''########################## IMAGE PRE-PROCESSING for TRAINING and TESTING data ##############################'''\n",
    "\n",
    "TrainingImagePath = 'C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data/casting_data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660d7391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# To Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56dc9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformation on raw images of training data\n",
    "# This ImageDataGenerator tries to generate more data than existed\n",
    "# train datagen will have these kind of the properties\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1/255, # Rescale to 255 pixels\n",
    "        shear_range=0.1, # Shear/cut it by 10%\n",
    "        zoom_range=0.1, # Zoom it by 10%\n",
    "        horizontal_flip=True) # Flip it horizontally as well,\n",
    "# It applies above operations on the available images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a17ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# In testing we are not performing any operation\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdbc89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6633 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating the Training Data generated by using TrainingDataGenerator object\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64), #Usually we have 331x331 pixels image which are more enough for the system to process.\n",
    "                            # Essentially these algorithms are helpful to preserve main characteristic of images, still\n",
    "                            # generate a good image, here we compress it and learn these 331 pixels\n",
    "                            # Now CNN has reinforced it, compress it and extracting and learning it..\n",
    "                            # Just to avoid CPU load, we are reducing the image spec, which is good in recognising\n",
    "                            # If we need, we can give 200x200 but it should be as lesser as possible\n",
    "        batch_size=32, # batch size is 32, in how many batches we are going to do operation\n",
    "        class_mode='categorical') # It maintains an index, face1->index 0, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff410513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6633 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64,64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6aab6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'def_front': 0, 'ok_front': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing class labels for each product photo\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141820cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efcd7da",
   "metadata": {},
   "source": [
    "### Creating a list of faces and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fbb2aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Part and its ID {0: 'def_front', 1: 'ok_front'}\n",
      "\\ The Number of output neurons:  2\n"
     ]
    }
   ],
   "source": [
    "'''################ Creating lookup table for all photos###########'''\n",
    "# class_indices have the numeric tag for each photo\n",
    "TrainClasses=training_set.class_indices\n",
    "\n",
    "# Storing the face and the numeric tag forr future reference\n",
    "ResultMap={}\n",
    "for partValue, partName in zip(TrainClasses.values(), TrainClasses.keys()):\n",
    "    ResultMap[partValue]=partName\n",
    "    \n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultMap.pkl\", 'wb') as f: # here we storing it\n",
    "    pickle.dump(ResultMap, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('Mapping of Part and its ID', ResultMap)\n",
    "\n",
    "# The number of neurons for the output layer is equal to the number of parts\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\ The Number of output neurons: ', OutputNeurons) # Output layer is determined by number of classes, 2 probabilities\n",
    "# No. of output neurons are 2, output produced by ANN is 2 probabilities out of which highest probabilities will win"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e977f",
   "metadata": {},
   "source": [
    "### Creating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db6d83e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''############## Create CNN Deep Learning model ##########'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    "\n",
    "''' STEP--1 CONVOLUTION\n",
    "# Adding the first layer of CNN\n",
    "# We are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64x64) pixels representing Red, Green and Blue components of pixels'''\n",
    "classifier.add(Convolution2D(filters=32, kernel_size=(3,3), strides=(1,1), input_shape=(64,64,3),# Means 3-> Matrices of 64x64\n",
    "                            activation='relu'))\n",
    "\n",
    "'''# STEP--2 MAX Pooling'''# Hyper parameter-> pool size which affects accuracy\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''############ ADDITIONAL LAYER of CONVOLUTION for better accuracy ##########'''\n",
    "# Additional round of convule/consize/reduce image size-> it extracts most important part of the image\n",
    "classifier.add(Convolution2D(filters=64, kernel_size=(5,5), strides=(2,2), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "'''# STEP--3 FLATTEING'''\n",
    "classifier.add(Flatten())\n",
    "\n",
    "'''# STEP--4 FULLY CONNECTED NEURAL NETWORK'''\n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    "\n",
    "'''# COMPILING THE CNN'''\n",
    "#classfier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "# crossentropy--> categorical_crossentropy for multiclass classification\n",
    "# binary_crossentropy--> for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a657b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f187f79",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7437dfe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7240\\201778491.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  classifier.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.7695 - accuracy: 0.4500 - val_loss: 0.7190 - val_accuracy: 0.4375\n",
      "Epoch 2/35\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.7040 - accuracy: 0.4875 - val_loss: 0.6762 - val_accuracy: 0.5625\n",
      "Epoch 3/35\n",
      "5/5 [==============================] - 4s 852ms/step - loss: 0.7063 - accuracy: 0.4812 - val_loss: 0.6908 - val_accuracy: 0.3906\n",
      "Epoch 4/35\n",
      "5/5 [==============================] - 4s 827ms/step - loss: 0.6830 - accuracy: 0.6438 - val_loss: 0.6808 - val_accuracy: 0.7031\n",
      "Epoch 5/35\n",
      "5/5 [==============================] - 4s 784ms/step - loss: 0.6849 - accuracy: 0.5437 - val_loss: 0.6776 - val_accuracy: 0.5312\n",
      "Epoch 6/35\n",
      "5/5 [==============================] - 4s 788ms/step - loss: 0.6687 - accuracy: 0.5938 - val_loss: 0.6406 - val_accuracy: 0.6562\n",
      "Epoch 7/35\n",
      "5/5 [==============================] - 4s 764ms/step - loss: 0.6830 - accuracy: 0.5375 - val_loss: 0.6723 - val_accuracy: 0.5938\n",
      "Epoch 8/35\n",
      "5/5 [==============================] - 3s 711ms/step - loss: 0.6648 - accuracy: 0.5938 - val_loss: 0.7009 - val_accuracy: 0.4844\n",
      "Epoch 9/35\n",
      "5/5 [==============================] - 3s 736ms/step - loss: 0.6600 - accuracy: 0.5625 - val_loss: 0.6616 - val_accuracy: 0.5312\n",
      "Epoch 10/35\n",
      "5/5 [==============================] - 3s 663ms/step - loss: 0.6466 - accuracy: 0.6250 - val_loss: 0.5854 - val_accuracy: 0.6719\n",
      "Epoch 11/35\n",
      "5/5 [==============================] - 3s 662ms/step - loss: 0.6424 - accuracy: 0.5875 - val_loss: 0.6211 - val_accuracy: 0.6406\n",
      "Epoch 12/35\n",
      "5/5 [==============================] - 3s 651ms/step - loss: 0.6310 - accuracy: 0.6687 - val_loss: 0.5869 - val_accuracy: 0.7344\n",
      "Epoch 13/35\n",
      "5/5 [==============================] - 3s 659ms/step - loss: 0.6105 - accuracy: 0.6812 - val_loss: 0.6294 - val_accuracy: 0.5781\n",
      "Epoch 14/35\n",
      "5/5 [==============================] - 3s 631ms/step - loss: 0.6891 - accuracy: 0.5250 - val_loss: 0.6308 - val_accuracy: 0.5781\n",
      "Epoch 15/35\n",
      "5/5 [==============================] - 3s 639ms/step - loss: 0.6324 - accuracy: 0.6187 - val_loss: 0.6223 - val_accuracy: 0.6562\n",
      "Epoch 16/35\n",
      "5/5 [==============================] - 3s 596ms/step - loss: 0.6353 - accuracy: 0.6062 - val_loss: 0.5880 - val_accuracy: 0.7031\n",
      "Epoch 17/35\n",
      "5/5 [==============================] - 3s 612ms/step - loss: 0.5885 - accuracy: 0.6875 - val_loss: 0.5984 - val_accuracy: 0.6094\n",
      "Epoch 18/35\n",
      "5/5 [==============================] - 3s 618ms/step - loss: 0.6415 - accuracy: 0.6562 - val_loss: 0.5981 - val_accuracy: 0.7500\n",
      "Epoch 19/35\n",
      "5/5 [==============================] - 3s 580ms/step - loss: 0.6025 - accuracy: 0.6750 - val_loss: 0.6084 - val_accuracy: 0.6562\n",
      "Epoch 20/35\n",
      "5/5 [==============================] - 3s 583ms/step - loss: 0.6097 - accuracy: 0.6500 - val_loss: 0.5284 - val_accuracy: 0.7812\n",
      "Epoch 21/35\n",
      "5/5 [==============================] - 3s 592ms/step - loss: 0.5452 - accuracy: 0.7312 - val_loss: 0.5752 - val_accuracy: 0.7656\n",
      "Epoch 22/35\n",
      "5/5 [==============================] - 3s 552ms/step - loss: 0.5854 - accuracy: 0.6313 - val_loss: 0.4543 - val_accuracy: 0.8906\n",
      "Epoch 23/35\n",
      "5/5 [==============================] - 3s 521ms/step - loss: 0.5357 - accuracy: 0.7500 - val_loss: 0.4616 - val_accuracy: 0.8438\n",
      "Epoch 24/35\n",
      "5/5 [==============================] - 3s 630ms/step - loss: 0.5797 - accuracy: 0.6625 - val_loss: 0.4060 - val_accuracy: 0.8594\n",
      "Epoch 25/35\n",
      "5/5 [==============================] - 3s 609ms/step - loss: 0.5768 - accuracy: 0.6812 - val_loss: 0.5164 - val_accuracy: 0.7656\n",
      "Epoch 26/35\n",
      "5/5 [==============================] - 3s 583ms/step - loss: 0.6037 - accuracy: 0.6750 - val_loss: 0.5134 - val_accuracy: 0.7812\n",
      "Epoch 27/35\n",
      "5/5 [==============================] - 3s 661ms/step - loss: 0.5523 - accuracy: 0.7000 - val_loss: 0.4924 - val_accuracy: 0.7812\n",
      "Epoch 28/35\n",
      "5/5 [==============================] - 3s 579ms/step - loss: 0.5650 - accuracy: 0.7625 - val_loss: 0.5674 - val_accuracy: 0.7500\n",
      "Epoch 29/35\n",
      "5/5 [==============================] - 3s 563ms/step - loss: 0.5709 - accuracy: 0.7375 - val_loss: 0.4932 - val_accuracy: 0.7812\n",
      "Epoch 30/35\n",
      "5/5 [==============================] - 3s 508ms/step - loss: 0.5178 - accuracy: 0.7563 - val_loss: 0.3929 - val_accuracy: 0.8750\n",
      "Epoch 31/35\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.5634 - accuracy: 0.7000 - val_loss: 0.4384 - val_accuracy: 0.8125\n",
      "Epoch 32/35\n",
      "5/5 [==============================] - 2s 494ms/step - loss: 0.5005 - accuracy: 0.7500 - val_loss: 0.5039 - val_accuracy: 0.7812\n",
      "Epoch 33/35\n",
      "5/5 [==============================] - 2s 465ms/step - loss: 0.4909 - accuracy: 0.7500 - val_loss: 0.3921 - val_accuracy: 0.8438\n",
      "Epoch 34/35\n",
      "5/5 [==============================] - 2s 496ms/step - loss: 0.6038 - accuracy: 0.7125 - val_loss: 0.5320 - val_accuracy: 0.7812\n",
      "Epoch 35/35\n",
      "5/5 [==============================] - 3s 526ms/step - loss: 0.5582 - accuracy: 0.7188 - val_loss: 0.4297 - val_accuracy: 0.8594\n",
      "############# Total Time Taken:  2 Minutes ########\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    "\n",
    "# Starting the model training\n",
    "classifier.fit_generator(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=5,\n",
    "                    epochs=35, # like batch size for you, change epoch size to optimize the accuracy\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=2)\n",
    "\n",
    "EndTime=time.time()\n",
    "print('############# Total Time Taken: ', round((EndTime-StartTime)/60), 'Minutes ########')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ab77e",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "060b3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data\\assets\n"
     ]
    }
   ],
   "source": [
    "### Saving the model\n",
    "classifier.save('C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b203d0",
   "metadata": {},
   "source": [
    "### Testing the model on a different part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f6f0e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]]\n",
      "{0: 'def_front', 1: 'ok_front'}\n",
      "########################################\n",
      "Prediction is:  ok_front\n"
     ]
    }
   ],
   "source": [
    "'''############## Making single predictions #############'''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "testImage='C:/Users/Lenovo/ALL Python ML notebooks/Deep Learning Notebooks/Casting Data/casting_data/test/ok_front/cast_ok_0_504.jpeg'\n",
    "test_image=image.load_img(testImage,target_size=(64,64)) # Converts image to array object like nuerical array\n",
    "test_image=image.img_to_array(test_image) # the image 64,64 has to be converted into array format\n",
    "\n",
    "test_image=np.expand_dims(test_image,axis=0) # 64x64x3 array is created\n",
    "# This image is at the same level, where we trained our model\n",
    "\n",
    "result=classifier.predict(test_image,verbose=0) # Passing test image to classifier using predict function\n",
    "# verbose is 0 which doesn't produce the log output\n",
    "\n",
    "print(result)\n",
    "print(ResultMap)\n",
    "\n",
    "print('####'*10)\n",
    "print('Prediction is: ', ResultMap[np.argmax(result)]) # which is the max value give me the location of the value\n",
    "# whichever is maximum, it gives the location of that value , from which we get index, result map of highest value will\n",
    "# give us the part number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b89c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
